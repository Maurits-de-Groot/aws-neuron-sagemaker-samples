{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4feac2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine tuning BERT base model from HuggingFace on Amazon SageMaker\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutotial uses the Hugging Face transformers and datasets libraries with Amazon SageMaker to fine-tune a pre-trained BERT base model on binary text classification.\n",
    "\n",
    "A [pre-trained model](https://huggingface.co/bert-base-uncased) is available in the transformers library from Hugging Face. Youâ€™ll be fine-tuning this pre-trained model using the [Amazon Reviews dataset](https://huggingface.co/datasets/amazon_reviews_multi) and classify the review into either positive or negative feedback.\n",
    "\n",
    "This Jupyter Notebook can run on any type of SageMaker Notebook instances with kernel (e.g. `ml.t3.medium` with `conda_pytorch_p38`). You can set up your SageMaker Notebook instance by following the [Get Started with Amazon SageMaker Notebook Instances](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170f6bc",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b1ff5",
   "metadata": {},
   "source": [
    "For SageMaker Studio, please make sure ipywidgets is installed and restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54f106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install ipywidgets\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e17d62",
   "metadata": {},
   "source": [
    "This tutorial requires the following pip packages:\n",
    "  + transformers\n",
    "  + datasets\n",
    " \n",
    "Please make sure SageMaker version is 2.116.0 or byond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0072542d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-cache-dir 'sagemaker>=2.116.0'\n",
    "!{sys.executable} -m pip install --upgrade --no-cache-dir 'boto3>=1.26.8' 'botocore>=1.29.12' s3fs\n",
    "!{sys.executable} -m pip install --upgrade --no-cache-dir torch==1.11.0\n",
    "!{sys.executable} -m pip install --upgrade --no-cache-dir transformers==4.16.2 datasets==2.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b0e3f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3bc86",
   "metadata": {},
   "source": [
    "### Create Sagemaker session\n",
    "Next, create a SageMaker session and define an execution role. Default role should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492c0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sess_bucket = sess.default_bucket()\n",
    "\n",
    "print(f'sagemaker role arn: {role}')\n",
    "print(f'sagemaker bucket: {sess_bucket}')\n",
    "print(f'sagemaker session region: {sess.boto_region_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16196f63",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "For this example, we will use the `datasets` library to download and preprocess the `amazon_review_multi` dataset from Hugging Face. In original dataset the review sentences are classified as 5 Ratings. We will treat it as binary classification task where the review sentences are classified as Positive or Negative. We will use the original dataset of 5 and 4 as Positive (LABEL_1), 2 and 1 as Negative (LABEL_0), and data with Ratings of 3 will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727cd45f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pre-trained model used\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'amazon_reviews_multi'\n",
    "\n",
    "# s3 key prefix\n",
    "s3_prefix = 'bert-base-uncased-amazon-reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d07082",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "dataset = load_dataset(dataset_name, 'en')\n",
    "dataset = dataset.remove_columns(['review_id', 'product_id', 'reviewer_id', 'review_title', 'language', 'product_category'])\n",
    "dataset = dataset.filter(lambda dataset: dataset['stars'] != 3)\n",
    "dataset = dataset.map(lambda dataset: {'labels': int(dataset['stars'] > 3)}, remove_columns=['stars'])\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review_body'], padding='max_length', max_length=128, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['review_body'])\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "# Reduced the dataset for testing purpose\n",
    "train_dataset = tokenized_datasets['train'].shuffle().select(range(4000))\n",
    "eval_dataset = tokenized_datasets['test'].shuffle().select(range(256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2faa2",
   "metadata": {},
   "source": [
    "Let's take a look one example to understand how original sentence is tokeninzed and encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee14b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = 100000\n",
    "print(dataset['train'][index])\n",
    "print('Tokenize:', tokenizer.tokenize(dataset['train']['review_body'][index]))\n",
    "print('Encode:', tokenizer.encode(dataset['train']['review_body'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a28235a",
   "metadata": {},
   "source": [
    "### Uploading dataset to S3 Bucket\n",
    "After we processed the dataset we are going to use the new FileSystem integration to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890fc66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save dataset to s3\n",
    "training_input_path = f's3://{sess_bucket}/{s3_prefix}/{dataset_name}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "test_input_path = f's3://{sess_bucket}/{s3_prefix}/{dataset_name}/test'\n",
    "eval_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ffce8",
   "metadata": {},
   "source": [
    "## Leveraging Neuron Persistent Cache\n",
    "PyTorch Neuron performs just-in-time compilation of graphs during execution. At every step if the traced graph varies from the previous executions, it is compiled by the neuron compiler.\n",
    "The compilation result can be saved in on-disk [Neuron Persistent Cache](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/neuron-caching.html) to avoid compilations across training runs. In order to keep such cache across SageMaker traing jobs, we add the mechanism to restore the cache from S3 at the begging of the training job and then store back to S3 at the end of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df501f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# If neuron compile cache does not exist in S3, create dummy file.\n",
    "if not sess.list_s3_files(sess_bucket, f'{s3_prefix}/neuron-compile-cache.tar.gz'):\n",
    "    pathlib.Path('./dummy').touch()\n",
    "    with tarfile.open('neuron-compile-cache.tar.gz', 'w:gz') as t:\n",
    "        t.add('dummy')\n",
    "    sess.upload_data('./neuron-compile-cache.tar.gz', sess_bucket, s3_prefix)\n",
    "\n",
    "s3_prefix_path = f's3://{sess_bucket}/{s3_prefix}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44511ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-tuning & starting Sagemaker Training Job\n",
    "In order to create a sagemaker training job we use [PyTorch Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#train-a-model-with-pytorch). The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "Note) SageMaker support for EC2 Trn1 instance is currently available only for PyTorch Estimator. HuggingFace Estimator will be available in future release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f590e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 10,               # number of training epochs\n",
    "                 'train_batch_size': 8,      # batch size for training\n",
    "                 'eval_batch_size': 8,       # batch size for evaluation\n",
    "                 'learning_rate': 5e-5,      # learning rate used during training\n",
    "                 'model_name':model_name,    # pre-trained model\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7055a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pt_estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='scripts',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    framework_version='1.11.0',\n",
    "    py_version='py38',\n",
    "    base_job_name=s3_prefix,\n",
    "    hyperparameters=hyperparameters,\n",
    "\n",
    "    distribution={\n",
    "        'torch_distributed': {\n",
    "            'enabled': True\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c861f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911250e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "    'test': test_input_path,\n",
    "    's3_prefix': s3_prefix_path  # To restore Neuron Persistent Cache archive from S3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe435f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# starting the train job with our uploaded datasets as input\n",
    "pt_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92abb2f",
   "metadata": {},
   "source": [
    "### Backup Neuron Persistent Cache into S3 for the following training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c911a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainingJobName = pt_estimator.latest_training_job.describe()['TrainingJobName']\n",
    "\n",
    "sess.download_data('./', sess_bucket, f'{TrainingJobName}/output/output.tar.gz')\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open('output.tar.gz', 'r') as t:\n",
    "    t.extractall('./output/')\n",
    "\n",
    "with tarfile.open('neuron-compile-cache.tar.gz', 'w:gz') as t:\n",
    "    t.add('./output/neuron-compile-cache', arcname='neuron-compile-cache')\n",
    "\n",
    "sess.upload_data('./neuron-compile-cache.tar.gz', sess_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d7e92f",
   "metadata": {},
   "source": [
    "## Quick check the prediction result.\n",
    "Let's quickly check to see if the fine-tuned model produces the expected prediction results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d23c8-97f5-49b0-ba1f-a6b183b8fc55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.download_data('./', sess_bucket, f'{TrainingJobName}/output/model.tar.gz')\n",
    "    \n",
    "with tarfile.open('model.tar.gz', 'r') as t:\n",
    "    t.extractall('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99ecfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model = './model/')\n",
    "\n",
    "print(classifier('This is the most amazing product I have ever seen. I love it.'))\n",
    "print(classifier('big disappointment. I will not use it.'))\n",
    "print(classifier('Wonderful product. I will let my friends know.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61eb616",
   "metadata": {},
   "source": [
    "### Congratulation ! You can see the expected results !"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
